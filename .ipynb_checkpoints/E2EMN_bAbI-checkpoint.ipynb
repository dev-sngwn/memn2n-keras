{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ICPS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "import sys\n",
    "import collections\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from keras.optimizers import *\n",
    "from keras.callbacks import *\n",
    "from keras.models import *\n",
    "from keras.layers import *\n",
    "from keras.initializers import *\n",
    "from keras.activations import *\n",
    "from keras_layer_normalization import LayerNormalization\n",
    "\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "d_model = 512\n",
    "bAbI_max_len = 12\n",
    "\n",
    "data_en_path = \"data/en/\"\n",
    "data_en_10k_path = \"data/en-10k/\"\n",
    "data_en_valid_path = \"data/en-valid/\"\n",
    "data_en_valid_10k_path = \"data/en-valid-10k/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sentence:\n",
    "    def __init__(self, num, sentence):\n",
    "        self.num = num\n",
    "        self.sentence = sentence\n",
    "\n",
    "class bAbISet:\n",
    "    def __init__(self, task=None, sentences=None, question=None, answer=None, supporting_num=None):\n",
    "        self.task = task\n",
    "        self.sentences = sentences\n",
    "        self.question = question\n",
    "        self.answer = answer\n",
    "        self.supporting_num = supporting_num\n",
    "        \n",
    "    def add_vec_data(self, vec_sentences, vec_question, vec_answer):\n",
    "        self.vec_sentences = vec_sentences\n",
    "        self.vec_question = vec_question\n",
    "        self.vec_answer = vec_answer\n",
    "        \n",
    "    def Print(self):\n",
    "        print(\"=============================================================\")\n",
    "        print(\">> Task: \", self.task)\n",
    "        print(\">> Sentences: \", len(self.sentences))\n",
    "        for sentence in self.sentences:\n",
    "            print(sentence.num, \": \", sentence.sentence)\n",
    "        print(\">> Question: \", self.question)\n",
    "        print(\">> Answer: \", self.answer)\n",
    "        print(\">> Supporting Fact: \", self.supporting_num)\n",
    "        print(\"=============================================================\")\n",
    "        \n",
    "    def Print_vec(self):\n",
    "        if self.vec_sentences is None:\n",
    "            return\n",
    "        \n",
    "        print(\"=============================================================\")\n",
    "        print(\">> Task: \", self.task)\n",
    "        print(\">> Sentences: \", len(self.sentences))\n",
    "        for sentence in self.vec_sentences:\n",
    "            print(sentence.num, \": \", sentence.sentence)\n",
    "        print(\">> Question: \", self.vec_question)\n",
    "        print(\">> Answer: \", self.vec_answer)\n",
    "        print(\">> Supporting Fact: \", self.supporting_num)\n",
    "        print(\"=============================================================\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class bAbIUtils:\n",
    "    def __init__(self, path):\n",
    "        self.files = os.listdir(path)\n",
    "        self.files = [os.path.join(path, f) for f in self.files]\n",
    "        \n",
    "    def make_question_set(self, sentences):\n",
    "        all_set = []\n",
    "        temp_set = []\n",
    "        for sentence in sentences:\n",
    "            temp_set.append(sentence.lower())\n",
    "            if sentence.find('\\t') != -1:\n",
    "                all_set.append(temp_set.copy())\n",
    "                temp_set.clear()\n",
    "                \n",
    "        return all_set\n",
    "                \n",
    "    def data_processing(self):\n",
    "        print(\"※ Data Processing...\")\n",
    "        all_data = []\n",
    "        for file in self.files:\n",
    "            \n",
    "            idx_start = file.find('_') + 1\n",
    "            idx_end = file[idx_start:].find('_')\n",
    "            task = file[idx_start:idx_start + idx_end]\n",
    "            \n",
    "            with open(file, 'r') as f:\n",
    "                data = f.readlines()\n",
    "                raw_set = self.make_question_set(data)\n",
    "                \n",
    "                for one_set in raw_set[:1]:\n",
    "                    Sentence_set = []\n",
    "                    for sentence in one_set:\n",
    "                        sentence = sentence.replace('\\n', '')\n",
    "                        \n",
    "                        idx = sentence.find(' ')\n",
    "                        idx_answer =  sentence.find('\\t')\n",
    "                        if idx_answer != -1:\n",
    "                            question = sentence[idx + 1:idx_answer]\n",
    "                            idx_answer_end = sentence[idx_answer + 1:].find('\\t')\n",
    "                            answer = sentence[idx_answer + 1: idx_answer + idx_answer_end + 1]\n",
    "                            supporting_num = sentence[idx_answer + idx_answer_end + 2:]\n",
    "                            \n",
    "                        else:\n",
    "                            Sentence_set.append(Sentence(sentence[:idx], sentence[idx + 1:]))\n",
    "                            \n",
    "                    all_data.append(bAbISet(task, Sentence_set, question, answer, supporting_num))\n",
    "                    \n",
    "        return all_data\n",
    "    \n",
    "    def make_dictionary(self, all_set):\n",
    "        print(\">> Make Dictionary...\")\n",
    "        \n",
    "        sentence_set = []\n",
    "        for one_set in all_set:\n",
    "            for Sentence in one_set.sentences:\n",
    "                sentence_set.append(Sentence.sentence)\n",
    "            sentence_set.append(one_set.question)\n",
    "            sentence_set.append(one_set.answer)\n",
    "        \n",
    "        words = []\n",
    "        for sentence in sentence_set:\n",
    "            tokens = nltk.word_tokenize(sentence)\n",
    "                        \n",
    "            words.extend(tokens)\n",
    "\n",
    "        words = collections.Counter(words)\n",
    "\n",
    "        dictionary = {}\n",
    "        dictionary['<PAD>'] = 0\n",
    "        dictionary['<UNK>'] = 1\n",
    "        dictionary['<EOS>'] = 2\n",
    "        dictionary['<S>'] = 3\n",
    "        idx = 4\n",
    "        for word in words.most_common():\n",
    "            if len(word[0]) > 0:\n",
    "                dictionary[word[0]] = idx\n",
    "                idx += 1\n",
    "            \n",
    "            if idx >= 20000: break;\n",
    "\n",
    "        return dictionary\n",
    "    \n",
    "    def vectorize_sentence(self, sentence, dictionary):\n",
    "        vec_sentence = []\n",
    "        #vec_sentence.append(dictionary['<S>'])\n",
    "        for word in nltk.word_tokenize(sentence):\n",
    "            if len(word) > 0:\n",
    "                if word in dictionary:\n",
    "                    vec_sentence.append(dictionary[word])\n",
    "                else:\n",
    "                    vec_sentence.append(dictionary['<UNK>'])\n",
    "        \n",
    "        for _ in range(len(vec_sentence), bAbI_max_len):\n",
    "            vec_sentence.append(dictionary['<PAD>'])\n",
    "        #vec_sentence.append(dictionary['<EOS>'])\n",
    "        \n",
    "        return vec_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "※ Data Processing...\n",
      ">> Make Dictionary...\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "bAbI_util = bAbIUtils(data_en_path)\n",
    "bAbI = bAbI_util.data_processing()\n",
    "dictionary = bAbI_util.make_dictionary(bAbI)\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "max_len = 0\n",
    "for one_set in bAbI:\n",
    "    vec_sentences = []\n",
    "    for sentence in one_set.sentences:\n",
    "        vec_sentence = bAbI_util.vectorize_sentence(sentence.sentence, dictionary)\n",
    "        vec_sentences.append(Sentence(sentence.num, vec_sentence))\n",
    "        \n",
    "    vec_question = bAbI_util.vectorize_sentence(one_set.question, dictionary)\n",
    "    vec_answer = bAbI_util.vectorize_sentence(one_set.answer, dictionary)\n",
    "    one_set.add_vec_data(vec_sentences, vec_question, vec_answer)\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "class E2EMN:\n",
    "    def __init__(self):\n",
    "        A = Embedding(bAbI_max_len, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:RNN]",
   "language": "python",
   "name": "conda-env-RNN-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
