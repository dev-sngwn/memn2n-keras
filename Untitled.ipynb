{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.11111111\n",
      "0.11111111\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from keras.optimizers import *\n",
    "from keras.callbacks import *\n",
    "from keras.models import *\n",
    "from keras.layers import *\n",
    "from keras.initializers import *\n",
    "from keras.activations import *\n",
    "from keras_layer_normalization import LayerNormalization\n",
    "\n",
    "logits = tf.stack(np.array([[[[0, 1, 2, 3, 4]]]], dtype=np.float32))\n",
    "labels = tf.stack(np.array([[[0]]], dtype=np.int32))\n",
    "\n",
    "# labels = tf.constant([[[[0, 1, 2]]]])\n",
    "# logits = tf.constant([[[1]]])\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    y_pred = np.array([[[0.00, 0.98, 0.012, 0.032]], [[0.00, 0.00, 0.98, 0.032]]])\n",
    "    y_true = np.array([0, 1, 2, 3, 2, 1, 6, 7, 2, 9, 0, 0, 0, 0, 0])\n",
    "    mask = tf.cast(tf.not_equal(y_true, 0), 'float32')\n",
    "    corr = K.cast(K.equal(K.cast(y_true, 'int32'), K.cast(K.argmax(y_pred, axis=-1), 'int32')), 'float32')\n",
    "    corr = K.sum(corr * mask, -1) / K.sum(mask, -1)\n",
    "    print(corr.eval())\n",
    "    corr = K.mean(corr)\n",
    "    #loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=labels, logits=logits)\n",
    "    print(sess.run(corr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 3, 4)\n",
      "[[[list([0, 1, 2, 3]) list([4, 5, 6, 7, 8]) list([8, 8, 8])\n",
      "   list([3, 4, 5])]\n",
      "  [list([1, 2, 3]) list([5, 6, 7, 8]) list([8, 8]) list([4, 5])]]]\n",
      "--\n",
      "[[[list([1, 2, 3]) list([5, 6, 7, 8]) list([8, 8]) list([4, 5])]\n",
      "  [list([123, 124, 3512]) list([425, 236, 1247, 1248]) list([8, 8])\n",
      "   list([4, 5])]]]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([[[[0, 1, 2, 3], [4, 5, 6, 7, 8], [8,8 , 8], [3, 4, 5]],\n",
    "               [[1, 2, 3], [5, 6, 7, 8], [8, 8], [4, 5]],\n",
    "               [[123,124, 3512], [425, 236, 1247, 1248], [8, 8], [4, 5]]]])\n",
    "print(x.shape)\n",
    "print(x[:, :-1])\n",
    "print(\"--\")\n",
    "print(x[:, 1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import Counter\n",
    "import os\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from past.builtins import xrange\n",
    "import os\n",
    "import pprint\n",
    "import tensorflow as tf\n",
    "\n",
    "from progress.bar import Bar\n",
    "\n",
    "class ProgressBar(Bar):\n",
    "    message = 'Loading'\n",
    "    fill = '#'\n",
    "    suffix = '%(percent).1f%% | ETA: %(eta)ds'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(fname, count, word2idx):\n",
    "    if os.path.isfile(fname):\n",
    "        with open(fname) as f:\n",
    "            lines = f.readlines()\n",
    "    else:\n",
    "        raise(\"[!] Data %s not found\" % fname)\n",
    "\n",
    "    words = []\n",
    "    for line in lines:\n",
    "        words.extend(line.split())\n",
    "\n",
    "    if len(count) == 0:\n",
    "        count.append(['<eos>', 0])\n",
    "\n",
    "    count[0][1] += len(lines)\n",
    "    count.extend(Counter(words).most_common())\n",
    "\n",
    "    if len(word2idx) == 0:\n",
    "        word2idx['<eos>'] = 0\n",
    "\n",
    "    for word, _ in count:\n",
    "        if word not in word2idx:\n",
    "            word2idx[word] = len(word2idx)\n",
    "\n",
    "    data = list()\n",
    "    for line in lines:\n",
    "        for word in line.split():\n",
    "            index = word2idx[word]\n",
    "            data.append(index)\n",
    "        data.append(word2idx['<eos>'])\n",
    "\n",
    "    print(\"Read %s words from %s\" % (len(data), fname))\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MemN2N(object):\n",
    "    def __init__(self, config, nwords, sess):\n",
    "        print(\"MEMN2N Generate\")\n",
    "        self.nwords = nwords\n",
    "        self.init_hid = config.init_hid\n",
    "        self.init_std = config.init_std\n",
    "        self.batch_size = config.batch_size\n",
    "        self.nepoch = config.nepoch\n",
    "        self.nhop = config.nhop\n",
    "        self.edim = config.edim\n",
    "        self.mem_size = config.mem_size\n",
    "        self.lindim = config.lindim\n",
    "        self.max_grad_norm = config.max_grad_norm\n",
    "\n",
    "        self.show = config.show\n",
    "        self.is_test = config.is_test\n",
    "        self.checkpoint_dir = config.checkpoint_dir\n",
    "\n",
    "        if not os.path.isdir(self.checkpoint_dir):\n",
    "            raise Exception(\" [!] Directory %s not found\" % self.checkpoint_dir)\n",
    "\n",
    "        self.input = tf.placeholder(tf.float32, [None, self.edim], name=\"input\")\n",
    "        self.time = tf.placeholder(tf.int32, [None, self.mem_size], name=\"time\")\n",
    "        self.target = tf.placeholder(tf.float32, [self.batch_size, self.nwords], name=\"target\")\n",
    "        self.context = tf.placeholder(tf.int32, [self.batch_size, self.mem_size], name=\"context\")\n",
    "\n",
    "        self.hid = []\n",
    "        self.hid.append(self.input)\n",
    "        self.share_list = []\n",
    "        self.share_list.append([])\n",
    "\n",
    "        self.lr = None\n",
    "        self.current_lr = config.init_lr\n",
    "        self.loss = None\n",
    "        self.step = None\n",
    "        self.optim = None\n",
    "\n",
    "        self.sess = sess\n",
    "        self.log_loss = []\n",
    "        self.log_perp = []\n",
    "\n",
    "    def build_memory(self):\n",
    "        print(\"MEMN2N Build Memory\")\n",
    "        self.global_step = tf.Variable(0, name=\"global_step\")\n",
    "\n",
    "        self.A = tf.Variable(tf.random_normal([self.nwords, self.edim], stddev=self.init_std))\n",
    "        self.B = tf.Variable(tf.random_normal([self.nwords, self.edim], stddev=self.init_std))\n",
    "        self.C = tf.Variable(tf.random_normal([self.edim, self.edim], stddev=self.init_std))\n",
    "\n",
    "        # Temporal Encoding\n",
    "        self.T_A = tf.Variable(tf.random_normal([self.mem_size, self.edim], stddev=self.init_std))\n",
    "        self.T_B = tf.Variable(tf.random_normal([self.mem_size, self.edim], stddev=self.init_std))\n",
    "\n",
    "        # m_i = sum A_ij * x_ij + T_A_i\n",
    "        Ain_c = tf.nn.embedding_lookup(self.A, self.context)\n",
    "        Ain_t = tf.nn.embedding_lookup(self.T_A, self.time)\n",
    "        Ain = tf.add(Ain_c, Ain_t)\n",
    "\n",
    "        # c_i = sum B_ij * u + T_B_i\n",
    "        Bin_c = tf.nn.embedding_lookup(self.B, self.context)\n",
    "        Bin_t = tf.nn.embedding_lookup(self.T_B, self.time)\n",
    "        Bin = tf.add(Bin_c, Bin_t)\n",
    "\n",
    "        for h in xrange(self.nhop):\n",
    "            self.hid3dim = tf.reshape(self.hid[-1], [-1, 1, self.edim])\n",
    "            Aout = tf.matmul(self.hid3dim, Ain, adjoint_b=True)\n",
    "            Aout2dim = tf.reshape(Aout, [-1, self.mem_size])\n",
    "            P = tf.nn.softmax(Aout2dim)\n",
    "\n",
    "            probs3dim = tf.reshape(P, [-1, 1, self.mem_size])\n",
    "            Bout = tf.matmul(probs3dim, Bin)\n",
    "            Bout2dim = tf.reshape(Bout, [-1, self.edim])\n",
    "\n",
    "            Cout = tf.matmul(self.hid[-1], self.C)\n",
    "            Dout = tf.add(Cout, Bout2dim)\n",
    "\n",
    "            self.share_list[0].append(Cout)\n",
    "\n",
    "            if self.lindim == self.edim:\n",
    "                self.hid.append(Dout)\n",
    "            elif self.lindim == 0:\n",
    "                self.hid.append(tf.nn.relu(Dout))\n",
    "            else:\n",
    "                F = tf.slice(Dout, [0, 0], [self.batch_size, self.lindim])\n",
    "                G = tf.slice(Dout, [0, self.lindim], [self.batch_size, self.edim-self.lindim])\n",
    "                K = tf.nn.relu(G)\n",
    "                self.hid.append(tf.concat(axis=1, values=[F, K]))\n",
    "\n",
    "    def build_model(self):\n",
    "        print(\"MEMN2N Build Model\")\n",
    "        self.build_memory()\n",
    "\n",
    "        self.W = tf.Variable(tf.random_normal([self.edim, self.nwords], stddev=self.init_std))\n",
    "        z = tf.matmul(self.hid[-1], self.W)\n",
    "\n",
    "        self.loss = tf.nn.softmax_cross_entropy_with_logits_v2(logits=z, labels=self.target)\n",
    "\n",
    "        self.lr = tf.Variable(self.current_lr)\n",
    "        self.opt = tf.train.GradientDescentOptimizer(self.lr)\n",
    "\n",
    "        params = [self.A, self.B, self.C, self.T_A, self.T_B, self.W]\n",
    "        grads_and_vars = self.opt.compute_gradients(self.loss,params)\n",
    "        clipped_grads_and_vars = [(tf.clip_by_norm(gv[0], self.max_grad_norm), gv[1]) \\\n",
    "                                   for gv in grads_and_vars]\n",
    "\n",
    "        inc = self.global_step.assign_add(1)\n",
    "        with tf.control_dependencies([inc]):\n",
    "            self.optim = self.opt.apply_gradients(clipped_grads_and_vars)\n",
    "\n",
    "        tf.global_variables_initializer().run()\n",
    "        self.saver = tf.train.Saver()\n",
    "\n",
    "    def train(self, data):\n",
    "        print(\"MEMN2N Train\")\n",
    "        N = int(math.ceil(len(data) / self.batch_size))\n",
    "        cost = 0\n",
    "\n",
    "        x = np.ndarray([self.batch_size, self.edim], dtype=np.float32)\n",
    "        time = np.ndarray([self.batch_size, self.mem_size], dtype=np.int32)\n",
    "        target = np.zeros([self.batch_size, self.nwords]) # one-hot-encoded\n",
    "        context = np.ndarray([self.batch_size, self.mem_size])\n",
    "\n",
    "        x.fill(self.init_hid)\n",
    "        for t in xrange(self.mem_size):\n",
    "            time[:,t].fill(t)\n",
    "\n",
    "        if self.show:\n",
    "            bar = ProgressBar('Train', max=N)\n",
    "\n",
    "        for idx in xrange(N):\n",
    "            if self.show: bar.next()\n",
    "            target.fill(0)\n",
    "            for b in xrange(self.batch_size):\n",
    "                m = random.randrange(self.mem_size, len(data))\n",
    "                target[b][data[m]] = 1\n",
    "                context[b] = data[m - self.mem_size:m]\n",
    "\n",
    "            _, loss, self.step = self.sess.run([self.optim,\n",
    "                                                self.loss,\n",
    "                                                self.global_step],\n",
    "                                                feed_dict={\n",
    "                                                    self.input: x,\n",
    "                                                    self.time: time,\n",
    "                                                    self.target: target,\n",
    "                                                    self.context: context})\n",
    "            cost += np.sum(loss)\n",
    "\n",
    "        if self.show: bar.finish()\n",
    "        return cost/N/self.batch_size\n",
    "\n",
    "    def test(self, data, label='Test'):\n",
    "        N = int(math.ceil(len(data) / self.batch_size))\n",
    "        cost = 0\n",
    "\n",
    "        x = np.ndarray([self.batch_size, self.edim], dtype=np.float32)\n",
    "        time = np.ndarray([self.batch_size, self.mem_size], dtype=np.int32)\n",
    "        target = np.zeros([self.batch_size, self.nwords]) # one-hot-encoded\n",
    "        context = np.ndarray([self.batch_size, self.mem_size])\n",
    "\n",
    "        x.fill(self.init_hid)\n",
    "        for t in xrange(self.mem_size):\n",
    "            time[:,t].fill(t)\n",
    "\n",
    "        if self.show:\n",
    "            bar = ProgressBar(label, max=N)\n",
    "\n",
    "        m = self.mem_size\n",
    "        for idx in xrange(N):\n",
    "            if self.show: bar.next()\n",
    "            target.fill(0)\n",
    "            for b in xrange(self.batch_size):\n",
    "                target[b][data[m]] = 1\n",
    "                context[b] = data[m - self.mem_size:m]\n",
    "                m += 1\n",
    "\n",
    "                if m >= len(data):\n",
    "                    m = self.mem_size\n",
    "\n",
    "            loss = self.sess.run([self.loss], feed_dict={self.input: x,\n",
    "                                                         self.time: time,\n",
    "                                                         self.target: target,\n",
    "                                                         self.context: context})\n",
    "            cost += np.sum(loss)\n",
    "\n",
    "        if self.show: bar.finish()\n",
    "        return cost/N/self.batch_size\n",
    "\n",
    "    def run(self, train_data, test_data):\n",
    "        print(\"MEMN2N Run\")\n",
    "        if not self.is_test:\n",
    "            print(\"Is not Test\")\n",
    "            for idx in xrange(self.nepoch):\n",
    "                train_loss = np.sum(self.train(train_data))\n",
    "                test_loss = np.sum(self.test(test_data, label='Validation'))\n",
    "\n",
    "                # Logging\n",
    "                self.log_loss.append([train_loss, test_loss])\n",
    "                self.log_perp.append([math.exp(train_loss), math.exp(test_loss)])\n",
    "\n",
    "                state = {\n",
    "                    'perplexity': math.exp(train_loss),\n",
    "                    'epoch': idx,\n",
    "                    'learning_rate': self.current_lr,\n",
    "                    'valid_perplexity': math.exp(test_loss)\n",
    "                }\n",
    "                print(state)\n",
    "\n",
    "                # Learning rate annealing\n",
    "                if len(self.log_loss) > 1 and self.log_loss[idx][1] > self.log_loss[idx-1][1] * 0.9999:\n",
    "                    self.current_lr = self.current_lr / 1.5\n",
    "                    self.lr.assign(self.current_lr).eval()\n",
    "                if self.current_lr < 1e-5: break\n",
    "\n",
    "                if idx % 10 == 0:\n",
    "                    self.saver.save(self.sess,\n",
    "                                    os.path.join(self.checkpoint_dir, \"MemN2N.model\"),\n",
    "                                    global_step = self.step.astype(int))\n",
    "        else:\n",
    "            self.load()\n",
    "\n",
    "            valid_loss = np.sum(self.test(train_data, label='Validation'))\n",
    "            test_loss = np.sum(self.test(test_data, label='Test'))\n",
    "\n",
    "            state = {\n",
    "                'valid_perplexity': math.exp(valid_loss),\n",
    "                'test_perplexity': math.exp(test_loss)\n",
    "            }\n",
    "            print(state)\n",
    "\n",
    "    def load(self):\n",
    "        print(\" [*] Reading checkpoints...\")\n",
    "        ckpt = tf.train.get_checkpoint_state(self.checkpoint_dir)\n",
    "        if ckpt and ckpt.model_checkpoint_path:\n",
    "            self.saver.restore(self.sess, ckpt.model_checkpoint_path)\n",
    "        else:\n",
    "            raise Exception(\" [!] Trest mode but no checkpoint found\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 929589 words from data/ptb.train.txt\n",
      "Read 73760 words from data/ptb.valid.txt\n",
      "Read 82430 words from data/ptb.test.txt\n",
      "{'batch_size': <absl.flags._flag.Flag object at 0x00000225A8B9C208>,\n",
      " 'checkpoint_dir': <absl.flags._flag.Flag object at 0x00000225A8B9C6D8>,\n",
      " 'data_dir': <absl.flags._flag.Flag object at 0x00000225A8B9C630>,\n",
      " 'data_name': <absl.flags._flag.Flag object at 0x00000225A8B9C7F0>,\n",
      " 'edim': <absl.flags._flag.Flag object at 0x00000225A8B9C0B8>,\n",
      " 'h': <tensorflow.python.platform.app._HelpFlag object at 0x00000225A8B9CA20>,\n",
      " 'help': <tensorflow.python.platform.app._HelpFlag object at 0x00000225A8B9CA20>,\n",
      " 'helpfull': <tensorflow.python.platform.app._HelpfullFlag object at 0x00000225A8B9CA90>,\n",
      " 'helpshort': <tensorflow.python.platform.app._HelpshortFlag object at 0x00000225A8B9CB00>,\n",
      " 'init_hid': <absl.flags._flag.Flag object at 0x00000225A8B9C400>,\n",
      " 'init_lr': <absl.flags._flag.Flag object at 0x00000225A8B9C4A8>,\n",
      " 'init_std': <absl.flags._flag.Flag object at 0x00000225A8B9C518>,\n",
      " 'is_test': <absl.flags._flag.BooleanFlag object at 0x00000225A8B9C8D0>,\n",
      " 'lindim': <absl.flags._flag.Flag object at 0x00000225A8B9C080>,\n",
      " 'max_grad_norm': <absl.flags._flag.Flag object at 0x00000225A8B9C588>,\n",
      " 'mem_size': <absl.flags._flag.Flag object at 0x00000225A8B9C160>,\n",
      " 'nepoch': <absl.flags._flag.Flag object at 0x00000225A8B9C2B0>,\n",
      " 'nhop': <absl.flags._flag.Flag object at 0x00000225A8B9C0F0>,\n",
      " 'show': <absl.flags._flag.BooleanFlag object at 0x00000225A8B9C978>}\n",
      "MEMN2N Generate\n",
      "MEMN2N Build Model\n",
      "MEMN2N Build Memory\n",
      "train start\n",
      "MEMN2N Run\n",
      "Is not Test\n",
      "MEMN2N Train\n",
      "{'perplexity': 645.0963033208968, 'epoch': 0, 'learning_rate': 0.01, 'valid_perplexity': 370.388567934417}\n",
      "MEMN2N Train\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-bacd0ca130ed>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'__main__'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 53\u001b[1;33m     \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\RNN\\lib\\site-packages\\tensorflow\\python\\platform\\app.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(main, argv)\u001b[0m\n\u001b[0;32m    123\u001b[0m   \u001b[1;31m# Call the main function, passing through any arguments\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    124\u001b[0m   \u001b[1;31m# to the final program.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 125\u001b[1;33m   \u001b[0m_sys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    126\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-bacd0ca130ed>\u001b[0m in \u001b[0;36mmain\u001b[1;34m(_)\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"train start\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 50\u001b[1;33m             \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     51\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'__main__'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-77256bd91325>\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, train_data, test_data)\u001b[0m\n\u001b[0;32m    190\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Is not Test\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    191\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mxrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnepoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 192\u001b[1;33m                 \u001b[0mtrain_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    193\u001b[0m                 \u001b[0mtest_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Validation'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    194\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-77256bd91325>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    142\u001b[0m                                                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m                                                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 144\u001b[1;33m                                                     self.context: context})\n\u001b[0m\u001b[0;32m    145\u001b[0m             \u001b[0mcost\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    146\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\RNN\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    898\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 900\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    901\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\RNN\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1102\u001b[0m             \u001b[0mfeed_handles\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msubfeed_t\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msubfeed_val\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1103\u001b[0m           \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1104\u001b[1;33m             \u001b[0mnp_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubfeed_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msubfeed_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1105\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1106\u001b[0m           if (not is_tensor_handle_feed and\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\RNN\\lib\\site-packages\\numpy\\core\\numeric.py\u001b[0m in \u001b[0;36masarray\u001b[1;34m(a, dtype, order)\u001b[0m\n\u001b[0;32m    490\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    491\u001b[0m     \"\"\"\n\u001b[1;32m--> 492\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    493\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    494\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "pp = pprint.PrettyPrinter()\n",
    "\n",
    "flags = tf.app.flags\n",
    "\n",
    "flags.DEFINE_integer(\"edim\", 150, \"internal state dimension [150]\")\n",
    "flags.DEFINE_integer(\"lindim\", 75, \"linear part of the state [75]\")\n",
    "flags.DEFINE_integer(\"nhop\", 6, \"number of hops [6]\")\n",
    "flags.DEFINE_integer(\"mem_size\", 100, \"memory size [100]\")\n",
    "flags.DEFINE_integer(\"batch_size\", 128, \"batch size to use during training [128]\")\n",
    "flags.DEFINE_integer(\"nepoch\", 100, \"number of epoch to use during training [100]\")\n",
    "flags.DEFINE_float(\"init_lr\", 0.01, \"initial learning rate [0.01]\")\n",
    "flags.DEFINE_float(\"init_hid\", 0.1, \"initial internal state value [0.1]\")\n",
    "flags.DEFINE_float(\"init_std\", 0.05, \"weight initialization std [0.05]\")\n",
    "flags.DEFINE_float(\"max_grad_norm\", 50, \"clip gradients to this norm [50]\")\n",
    "flags.DEFINE_string(\"data_dir\", \"data\", \"data directory [data]\")\n",
    "flags.DEFINE_string(\"checkpoint_dir\", \"checkpoints\", \"checkpoint directory [checkpoints]\")\n",
    "flags.DEFINE_string(\"data_name\", \"ptb\", \"data set name [ptb]\")\n",
    "flags.DEFINE_boolean(\"is_test\", False, \"True for testing, False for Training [False]\")\n",
    "flags.DEFINE_boolean(\"show\", True, \"print progress [False]\")\n",
    "\n",
    "FLAGS = flags.FLAGS\n",
    "\n",
    "def main(_):\n",
    "    count = []\n",
    "    word2idx = {}\n",
    "\n",
    "    if not os.path.exists(FLAGS.checkpoint_dir):\n",
    "      os.makedirs(FLAGS.checkpoint_dir)\n",
    "\n",
    "    train_data = read_data('%s/%s.train.txt' % (FLAGS.data_dir, FLAGS.data_name), count, word2idx)\n",
    "    valid_data = read_data('%s/%s.valid.txt' % (FLAGS.data_dir, FLAGS.data_name), count, word2idx)\n",
    "    test_data = read_data('%s/%s.test.txt' % (FLAGS.data_dir, FLAGS.data_name), count, word2idx)\n",
    "\n",
    "    idx2word = dict(zip(word2idx.values(), word2idx.keys()))\n",
    "    #FLAGS.nwords = len(word2idx)\n",
    "\n",
    "    pp.pprint(flags.FLAGS.__flags)\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        model = MemN2N(FLAGS, len(word2idx), sess)\n",
    "        model.build_model()\n",
    "\n",
    "        if FLAGS.is_test:\n",
    "            print(\"test start\")\n",
    "            model.run(valid_data, test_data)\n",
    "        else:\n",
    "            print(\"train start\")\n",
    "            model.run(train_data, valid_data)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    tf.app.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:RNN]",
   "language": "python",
   "name": "conda-env-RNN-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
